{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation - Transformer Version\n",
    "try out transformer based model for our classification task\n",
    "this is based on https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb\n",
    "\n",
    "## Approach\n",
    "As transformer are current state of the art for many nlp tasks I wanted to try this out for our multi-label-classification task. \n",
    "\n",
    "This model I based on pretrained https://huggingface.co/bert-base-uncased model.\n",
    "I gave it only one shot as training is quite expensive and took on my laptop with one RTX 3080 GPU more than XX hours.\n",
    "\n",
    "Most of the code is taken/adopted from other sources which I linked here in this notebook. Only the function train_test_val_split which takes a pandas Dataframe as input and returns a DatasetDict has been implemented by me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as DataFrame\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0        0      0            0             0                 0  ...   \n",
       "1        0      0            1             0                 0  ...   \n",
       "2        0      0            0             0                 0  ...   \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "\n",
       "[3 rows x 40 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///data/DisasterResponse.db')\n",
    "df = pd.read_sql_query('select * from CleanedData', engine)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4  12  says: west side of Haiti, rest of the country ...   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct     True   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct     True   \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct     True   \n",
       "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct     True   \n",
       "4  facade ouest d Haiti et le reste du pays aujou...  direct     True   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0    False  False        False         False             False  ...   \n",
       "1    False  False         True         False             False  ...   \n",
       "2    False  False        False         False             False  ...   \n",
       "3     True  False         True         False              True  ...   \n",
       "4    False  False        False         False             False  ...   \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm   fire  \\\n",
       "0        False                 False            False   False  False  False   \n",
       "1        False                 False             True   False   True  False   \n",
       "2        False                 False            False   False  False  False   \n",
       "3        False                 False            False   False  False  False   \n",
       "4        False                 False            False   False  False  False   \n",
       "\n",
       "   earthquake   cold  other_weather  direct_report  \n",
       "0       False  False          False          False  \n",
       "1       False  False          False          False  \n",
       "2       False  False          False          False  \n",
       "3       False  False          False          False  \n",
       "4       False  False          False          False  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make categorie values boolean\n",
    "\n",
    "categories = df.columns[4:]\n",
    "for column in categories:\n",
    "    df[column] = df[column].astype(bool)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12413/3649287926.py:2: FutureWarning: The default value of numeric_only in DataFrame.max is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  len(df.max()['message'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is max, min and avg lenght of message?\n",
    "len(df.max()['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  related  request  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...     True    False   \n",
       "\n",
       "   offer  aid_related  medical_help  medical_products  search_and_rescue  \\\n",
       "0  False        False         False             False              False   \n",
       "\n",
       "   security  ...  aid_centers  other_infrastructure  weather_related  floods  \\\n",
       "0     False  ...        False                 False            False   False   \n",
       "\n",
       "   storm   fire  earthquake   cold  other_weather  direct_report  \n",
       "0  False  False       False  False          False          False  \n",
       "\n",
       "[1 rows x 38 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop columns\n",
    "df_cleaned = df.drop( ['original', 'genre'], axis = 1)\n",
    "df_cleaned.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, message, related, request, offer, aid_related, medical_help, medical_products, search_and_rescue, security, military, child_alone, water, food, shelter, clothing, money, missing_people, refugees, death, other_aid, infrastructure_related, transport, buildings, electricity, tools, hospitals, shops, aid_centers, other_infrastructure, weather_related, floods, storm, fire, earthquake, cold, other_weather, direct_report]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 38 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows with nan\n",
    "df_nan=df_cleaned[df_cleaned.isna().any(axis=1)]\n",
    "df_nan.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26216, 38)\n",
      "(26216, 38)\n"
     ]
    }
   ],
   "source": [
    "# NaN values\n",
    "print(df_cleaned.shape)\n",
    "df_cleaned.dropna(inplace=True)\n",
    "print(df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['offer', 'aid_related', 'medical_help', 'medical_products',\n",
       "       'search_and_rescue', 'security', 'military', 'child_alone', 'water',\n",
       "       'food', 'shelter', 'clothing', 'money', 'missing_people', 'refugees',\n",
       "       'death', 'other_aid', 'infrastructure_related', 'transport',\n",
       "       'buildings', 'electricity', 'tools', 'hospitals', 'shops',\n",
       "       'aid_centers', 'other_infrastructure', 'weather_related', 'floods',\n",
       "       'storm', 'fire', 'earthquake', 'cold', 'other_weather',\n",
       "       'direct_report'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned[df_cleaned.columns[4:]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(df_cleaned))\n",
    "test_size = int(0.1 * len(df_cleaned))\n",
    "val_size = len(df_cleaned) - train_size - test_size\n",
    "random_seed = 43\n",
    "df_randomized = df_cleaned.sample(frac=1, random_state=random_seed)\n",
    "validate, test, train = np.split(df_cleaned, [test_size, test_size + val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: (26216, 40)\n",
      "test: (2623, 38)\n",
      "train: (20972, 38)\n",
      "validate: (2621, 38)\n"
     ]
    }
   ],
   "source": [
    "print(f'df: {df.shape}')\n",
    "print(f'test: {test.shape}')\n",
    "print(f'train: {train.shape}')\n",
    "print(f'validate: {validate.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test and Validation split into Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(df: DataFrame, train_perc = 0.8, test_perc = 0.1, random_seed = 42, max_size=None, drop_cats=None):\n",
    "    ''' returns a randomized split of df into train, test and validation DataSet(s)\n",
    "\n",
    "    INPUT:\n",
    "        df            - DataFrame\n",
    "        train_perc    - percentage of data used for training\n",
    "        test_perd     - percentage of data used for testing\n",
    "        random_seed   - random seed\n",
    "        max_size      - if set, only max_size entries from df will be used\n",
    "        \n",
    "    \n",
    "    OUTPUT:\n",
    "        data_set_dict - DatasetDict with train, test and validation data\n",
    "    ''' \n",
    "    \n",
    "    if drop_cats:\n",
    "        df = df.drop(df.columns[-drop_cats:],axis=1)\n",
    "        \n",
    "    df_randomized = df.sample(frac=1, random_state=random_seed)\n",
    "    \n",
    "    if max_size:\n",
    "        df_randomized= df_randomized[:max_size]\n",
    "    \n",
    "    train_size = int(train_perc * len(df_randomized))\n",
    "    test_size = int(test_perc * len(df_randomized))\n",
    "    val_size = len(df_randomized) - train_size - test_size\n",
    "    \n",
    "    df_validate, df_test, df_train = np.split(df_randomized, [test_size, test_size + val_size])\n",
    "\n",
    "    train_dataset = Dataset.from_dict(df_train)\n",
    "    test_dataset  = Dataset.from_dict(df_test)\n",
    "    val_dataset   = Dataset.from_dict(df_validate)\n",
    "    my_dataset_dict = datasets.DatasetDict({\"train\":train_dataset,\"test\":test_dataset, \"validation\": val_dataset})\n",
    "\n",
    "    return my_dataset_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26216, 38)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'message', 'related', 'request', 'offer', 'aid_related', 'medical_help', 'medical_products', 'search_and_rescue', 'security', 'military', 'child_alone', 'water', 'food', 'shelter', 'clothing', 'money', 'missing_people', 'refugees', 'death', 'other_aid', 'infrastructure_related', 'transport', 'buildings', 'electricity', 'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure', 'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold', 'other_weather', 'direct_report'],\n",
       "        num_rows: 20972\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'message', 'related', 'request', 'offer', 'aid_related', 'medical_help', 'medical_products', 'search_and_rescue', 'security', 'military', 'child_alone', 'water', 'food', 'shelter', 'clothing', 'money', 'missing_people', 'refugees', 'death', 'other_aid', 'infrastructure_related', 'transport', 'buildings', 'electricity', 'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure', 'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold', 'other_weather', 'direct_report'],\n",
       "        num_rows: 2623\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'message', 'related', 'request', 'offer', 'aid_related', 'medical_help', 'medical_products', 'search_and_rescue', 'security', 'military', 'child_alone', 'water', 'food', 'shelter', 'clothing', 'money', 'missing_people', 'refugees', 'death', 'other_aid', 'infrastructure_related', 'transport', 'buildings', 'electricity', 'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure', 'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold', 'other_weather', 'direct_report'],\n",
       "        num_rows: 2621\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dataset with trainint/test and validation data\n",
    "\n",
    "# dataset = train_test_val_split(df_cleaned, max_size=1000, drop_cats=30)\n",
    "dataset = train_test_val_split(df_cleaned)\n",
    "dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 27657,\n",
       " 'message': '**Provisional conclusions and recommendations** At the end of his visit the Special Rapporteur said: \"Burkina Faso has so far escaped the threat of terrorist attack, the spread of armed conflict across its borders, and the religious intolerance, radicalization and violent extremism among its population.',\n",
       " 'related': True,\n",
       " 'request': False,\n",
       " 'offer': False,\n",
       " 'aid_related': True,\n",
       " 'medical_help': False,\n",
       " 'medical_products': False,\n",
       " 'search_and_rescue': False,\n",
       " 'security': False,\n",
       " 'military': True,\n",
       " 'child_alone': False,\n",
       " 'water': False,\n",
       " 'food': False,\n",
       " 'shelter': False,\n",
       " 'clothing': False,\n",
       " 'money': False,\n",
       " 'missing_people': False,\n",
       " 'refugees': False,\n",
       " 'death': False,\n",
       " 'other_aid': False,\n",
       " 'infrastructure_related': False,\n",
       " 'transport': False,\n",
       " 'buildings': False,\n",
       " 'electricity': False,\n",
       " 'tools': False,\n",
       " 'hospitals': False,\n",
       " 'shops': False,\n",
       " 'aid_centers': False,\n",
       " 'other_infrastructure': False,\n",
       " 'weather_related': False,\n",
       " 'floods': False,\n",
       " 'storm': False,\n",
       " 'fire': False,\n",
       " 'earthquake': False,\n",
       " 'cold': False,\n",
       " 'other_weather': False,\n",
       " 'direct_report': False}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset['train'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['related',\n",
       " 'request',\n",
       " 'offer',\n",
       " 'aid_related',\n",
       " 'medical_help',\n",
       " 'medical_products',\n",
       " 'search_and_rescue',\n",
       " 'security',\n",
       " 'military',\n",
       " 'child_alone',\n",
       " 'water',\n",
       " 'food',\n",
       " 'shelter',\n",
       " 'clothing',\n",
       " 'money',\n",
       " 'missing_people',\n",
       " 'refugees',\n",
       " 'death',\n",
       " 'other_aid',\n",
       " 'infrastructure_related',\n",
       " 'transport',\n",
       " 'buildings',\n",
       " 'electricity',\n",
       " 'tools',\n",
       " 'hospitals',\n",
       " 'shops',\n",
       " 'aid_centers',\n",
       " 'other_infrastructure',\n",
       " 'weather_related',\n",
       " 'floods',\n",
       " 'storm',\n",
       " 'fire',\n",
       " 'earthquake',\n",
       " 'cold',\n",
       " 'other_weather',\n",
       " 'direct_report']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for label in dataset['train'].features.keys() if label not in ['id', 'message']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the DataSet for training\n",
    "Models like BERT don't allow for text as direct input. We have to tokenize the messages.\n",
    "Also the labels have to be of certain format for multi-label text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained models\n",
    "BERT_SINGLE = \"bert-base-uncased\"\n",
    "BERT_MULTI  = \"bert-base-multilingual-cased\"\n",
    "MODEL_PRET  = BERT_MULTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:01<00:00, 11.80ba/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 16.67ba/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 16.05ba/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PRET)\n",
    "\n",
    "def preprocess_data(examples):\n",
    "  ''' - tokenize text using AutoTokenizer API. \n",
    "      - provide labels (). For multi-label text classification, \n",
    "              this is a matrix of shape (batch_size, num_labels)\n",
    "              will be a tensor of floats rather integers \n",
    "              (see: https://discuss.pytorch.org/t/multi-label-binary-classification-result-type-float-cant-be-cast-to-the-desired-output-type-long/117915)\n",
    "    \n",
    "    INPUT:\n",
    "        examples      - DataSet\n",
    "    \n",
    "    OUTPUT:\n",
    "        encoding - encoding of text (from column 'message') and labels\n",
    "    ''' \n",
    "\n",
    "  # print(f'examples: {examples}')\n",
    "  # take a batch of texts\n",
    "  text = examples['message']\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 20972\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2623\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2621\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "example = encoded_dataset['train'][0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train'][0]['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([[101, 1008, 1008, 10864, 15306, 1998, 11433, 1008, 1008, 2012, 1996, 2203, 1997, 2010, 3942, 1996, 2569, 9680, 6442, 11236, 2056, 1024, 1000, 23089, 22773, 2038, 2061, 2521, 6376, 1996, 5081, 1997, 9452, 2886, 1010, 1996, 3659, 1997, 4273, 4736, 2408, 2049, 6645, 1010, 1998, 1996, 3412, 2046, 3917, 6651, 1010, 7490, 3989, 1998, 6355, 4654, 7913, 26725, 2426, 2049, 2313, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])\n"
     ]
    }
   ],
   "source": [
    "print(example.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['related', 'aid_related', 'military']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "We construct a model which is based on pretrained bert-base-uncased model (see https://huggingface.co/bert-base-uncased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PRET, \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "All training hyperparameters can be found here: https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    ''' - compute metrics while training \n",
    "        - code taken from: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "        - for details refer to this above url\n",
    "     \n",
    "    \n",
    "    OUTPUT:\n",
    "        metrics     - dictionary with the desired metric values\n",
    "    ''' \n",
    "    # print(f'predictions: {predictions}')\n",
    "    # print(f'labels: {labels}')\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    # print(f'y_true: {y_true}')\n",
    "    # print(f'y_pred: {y_pred}')\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred, normalize=True)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    ''' - compute multi-label metrics while training \n",
    "        - code taken from: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "        - for details refer to this above url\n",
    "     \n",
    "    \n",
    "    OUTPUT:\n",
    "        result   - dictionary with the desired metric values\n",
    "    ''' \n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train'][0]['labels'].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1008,  1008, 10864, 15306,  1998, 11433,  1008,  1008,  2012,\n",
       "         1996,  2203,  1997,  2010,  3942,  1996,  2569,  9680,  6442, 11236,\n",
       "         2056,  1024,  1000, 23089, 22773,  2038,  2061,  2521,  6376,  1996,\n",
       "         5081,  1997,  9452,  2886,  1010,  1996,  3659,  1997,  4273,  4736,\n",
       "         2408,  2049,  6645,  1010,  1998,  1996,  3412,  2046,  3917,  6651,\n",
       "         1010,  7490,  3989,  1998,  6355,  4654,  7913, 26725,  2426,  2049,\n",
       "         2313,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train'][0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.7102, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[ 0.2376, -0.0304,  0.0270, -0.3140, -0.3453,  0.0355, -0.4119, -0.1333,\n",
       "          0.0994,  0.5138,  0.1859, -0.0223, -0.3160, -0.1217,  0.1646, -0.4515,\n",
       "          0.1971, -0.0786, -0.0135,  0.1819,  0.6735, -0.2587,  0.0996,  0.5891,\n",
       "         -0.1681, -0.1149,  0.6752,  0.9858, -0.2200, -0.5146, -0.1499, -0.4692,\n",
       "          0.4473, -0.6613, -0.6028,  0.2837]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0), labels=encoded_dataset['train'][0]['labels'].unsqueeze(0))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heiko/miniconda3/envs/test/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20972\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 13110\n",
      "  Number of trainable parameters = 109509924\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13110' max='13110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13110/13110 13:41:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.144200</td>\n",
       "      <td>0.140213</td>\n",
       "      <td>0.686292</td>\n",
       "      <td>0.789811</td>\n",
       "      <td>0.309424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.123100</td>\n",
       "      <td>0.133238</td>\n",
       "      <td>0.704739</td>\n",
       "      <td>0.799871</td>\n",
       "      <td>0.361312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>0.133185</td>\n",
       "      <td>0.725353</td>\n",
       "      <td>0.826662</td>\n",
       "      <td>0.354063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>0.137308</td>\n",
       "      <td>0.721220</td>\n",
       "      <td>0.824603</td>\n",
       "      <td>0.362839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.084800</td>\n",
       "      <td>0.138986</td>\n",
       "      <td>0.720162</td>\n",
       "      <td>0.823474</td>\n",
       "      <td>0.363602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2621\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-2622\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-2622/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-2622/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-2622/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-2622/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2621\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-5244\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-5244/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-5244/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-5244/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-5244/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2621\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-7866\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-7866/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-7866/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-7866/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-7866/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2621\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-10488\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-10488/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-10488/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-10488/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-10488/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2621\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-13110\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-13110/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-13110/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-13110/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-13110/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from bert-finetuned-sem_eval-english/checkpoint-7866 (score: 0.7253534892812927).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13110, training_loss=0.11783118629892031, metrics={'train_runtime': 49292.3256, 'train_samples_per_second': 2.127, 'train_steps_per_second': 0.266, 'total_flos': 6899561918484480.0, 'train_loss': 0.11783118629892031, 'epoch': 5.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2621\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.1331845372915268,\n",
       " 'eval_f1': 0.7253534892812927,\n",
       " 'eval_roc_auc': 0.8266616602450627,\n",
       " 'eval_accuracy': 0.3540633346051126,\n",
       " 'eval_runtime': 242.8744,\n",
       " 'eval_samples_per_second': 10.792,\n",
       " 'eval_steps_per_second': 1.35,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model on Single Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"There is heavy rain and earth is shaking - need help!\"\n",
    "text = \"My family got lost and I have no food or even water\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "outputs = trainer.model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 36])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits is a tensor containing the scores for each label\n",
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['related', 'request', 'aid_related', 'water', 'food', 'direct_report']\n"
     ]
    }
   ],
   "source": [
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model for reuse\n",
    "save the model, so we can reload it using from_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to transormer_model\n",
      "Configuration saved in transormer_model/config.json\n",
      "Model weights saved in transormer_model/pytorch_model.bin\n",
      "tokenizer config file saved in transormer_model/tokenizer_config.json\n",
      "Special tokens file saved in transormer_model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('transormer_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check loading/using the model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./transormer_multi_model/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./transormer_multi_model/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_message(model, tokenizer, msg):\n",
    "    ''' Loads transformer based tokenizer and model. Evaluates input on this.\n",
    "        INPUT:\n",
    "            model     - trained transformer model\n",
    "            tokenizer - tokenizer for encoding the input message\n",
    "            msg       - single text message to be evaluated\n",
    "\n",
    "        \n",
    "        OUTPUT:\n",
    "            list of predicted categories\n",
    "    '''\n",
    "    text = msg\n",
    "\n",
    "    encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "    encoding = {k: v.to(model.device) for k,v in encoding.items()}\n",
    "\n",
    "    outputs = model(**encoding)\n",
    "    # logits is a tensor containing the scores for each label\n",
    "    logits = outputs.logits\n",
    "    logits.shape\n",
    "    # apply sigmoid + threshold\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(logits.squeeze().cpu())\n",
    "    predictions = np.zeros(probs.shape)\n",
    "    predictions[np.where(probs >= 0.5)] = 1\n",
    "    # turn predicted id's into actual label names\n",
    "    predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: 'related',\n",
    " 1: 'request',\n",
    " 2: 'offer',\n",
    " 3: 'aid_related',\n",
    " 4: 'medical_help',\n",
    " 5: 'medical_products',\n",
    " 6: 'search_and_rescue',\n",
    " 7: 'security',\n",
    " 8: 'military',\n",
    " 9: 'child_alone',\n",
    " 10: 'water',\n",
    " 11: 'food',\n",
    " 12: 'shelter',\n",
    " 13: 'clothing',\n",
    " 14: 'money',\n",
    " 15: 'missing_people',\n",
    " 16: 'refugees',\n",
    " 17: 'death',\n",
    " 18: 'other_aid',\n",
    " 19: 'infrastructure_related',\n",
    " 20: 'transport',\n",
    " 21: 'buildings',\n",
    " 22: 'electricity',\n",
    " 23: 'tools',\n",
    " 24: 'hospitals',\n",
    " 25: 'shops',\n",
    " 26: 'aid_centers',\n",
    " 27: 'other_infrastructure',\n",
    " 28: 'weather_related',\n",
    " 29: 'floods',\n",
    " 30: 'storm',\n",
    " 31: 'fire',\n",
    " 32: 'earthquake',\n",
    " 33: 'cold',\n",
    " 34: 'other_weather',\n",
    " 35: 'direct_report'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['related', 'weather_related']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim AusfÃ¼hren von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestÃ¼rzt. Bitte Ã¼berprÃ¼fen Sie den Code in der/den Zelle(n), um eine mÃ¶gliche Fehlerursache zu identifizieren. Klicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. Weitere Details finden Sie in Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "eval_message(model, tokenizer, 'Hier ist eine Flut und die Wassermassen reiÃŸen HÃ¤user ein')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb0fefe6d16f3ce0abbfad694f17c704a737f99cabcb5894bde1907b17c8bdac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
